{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Apache Beam on Azure Synpase\r\n",
        "\r\n",
        "Install Apache Beam and dependencies\r\n",
        "\r\n",
        "```\r\n",
        "apache-beam[interactive]\r\n",
        "pyarrow\r\n",
        "pandas==1.0.0\r\n",
        "```\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\r\n",
        "\r\n",
        "with beam.Pipeline() as pipeline:\r\n",
        "  lines = (\r\n",
        "      pipeline\r\n",
        "      | beam.Create([\r\n",
        "          'To be, or not to be: that is the question: ',\r\n",
        "          \"Whether 'tis nobler in the mind to suffer \",\r\n",
        "          'The slings and arrows of outrageous fortune, ',\r\n",
        "          'Or to take arms against a sea of troubles, ',\r\n",
        "      ])\r\n",
        "      )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Azure Storage Connection string"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "connection_string = \"\"\r\n",
        "os.environ['AZURE_STORAGE_CONNECTION_STRING'] = connection_string"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import apache_beam as beam\r\n",
        "from apache_beam.io.azure.blobstoragefilesystem import BlobStorageFileSystem\r\n",
        "\r\n",
        "\r\n",
        "sampleText = 'azfs://hyundevsynapsestorage/dev-synapse/user/trusted-service-user/beam/sample/sample.txt'\r\n",
        "outputPath = 'azfs://hyundevsynapsestorage/dev-synapse/user/trusted-service-user/beam/output/wordcount.txt'\r\n",
        "\r\n",
        "\r\n",
        "with beam.Pipeline() as pipeline:\r\n",
        "  lines = (\r\n",
        "      pipeline\r\n",
        "      | beam.io.textio.ReadFromText(sampleText)\r\n",
        "      | beam.FlatMap(lambda line: re.findall(r\"[a-zA-Z']+\", line))\r\n",
        "      | beam.Map(lambda word: (word, 1))\r\n",
        "      | beam.CombinePerKey(sum)\r\n",
        "      | beam.Map(lambda word_count: str(word_count))\r\n",
        "      | beam.io.textio.WriteToText(outputPath)\r\n",
        "      )\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outputPath = 'azfs://hyundevsynapsestorage/dev-synapse/user/trusted-service-user/beam/output/hamlet.txt'\r\n",
        "\r\n",
        "with beam.Pipeline() as pipeline:\r\n",
        "  lines = (\r\n",
        "      pipeline\r\n",
        "      | beam.Create([\r\n",
        "          'To be, or not to be: that is the question: ',\r\n",
        "          \"Whether 'tis nobler in the mind to suffer \",\r\n",
        "          'The slings and arrows of outrageous fortune, ',\r\n",
        "          'Or to take arms against a sea of troubles, ',\r\n",
        "      ])\r\n",
        "      | beam.io.textio.WriteToText(outputPath)\r\n",
        "      )\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sampledata_path = 'azfs://hyundevsynapsestorage/dev-synapse/data/bingCOVID19/*'\r\n",
        "\r\n",
        "# with beam.Pipeline() as pipeline:\r\n",
        "#   lines = (\r\n",
        "#       pipeline\r\n",
        "#       | beam.io.parquetio.ReadFromParquet(sampledata_path)\r\n",
        "#       | beam.Map(lambda table: table.to_pandas())\r\n",
        "#       )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyarrow \r\n",
        "\r\n",
        "outputdata_path = 'azfs://hyundevsynapsestorage/dev-synapse/data/beamoutput/outputsample.parquet'\r\n",
        "with beam.Pipeline() as p:\r\n",
        "  records = p | 'Read' >> beam.Create(\r\n",
        "      [{'name': 'foo', 'age': 10}, {'name': 'bar', 'age': 20}]\r\n",
        "  )\r\n",
        "  _ = records | 'Write' >> beam.io.WriteToParquet(outputdata_path,\r\n",
        "      pyarrow.schema(\r\n",
        "          [('name', pyarrow.binary()), ('age', pyarrow.int64())]\r\n",
        "      )\r\n",
        "  )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%pyspark\r\n",
        "df = spark.read.load([\r\n",
        "    'abfss://dev-synapse@hyundevsynapsestorage.dfs.core.windows.net/data/beamoutput/outputsample.parquet-00000-of-00001'\r\n",
        "    ], format='parquet')\r\n",
        "display(df.head(2))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "python"
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}